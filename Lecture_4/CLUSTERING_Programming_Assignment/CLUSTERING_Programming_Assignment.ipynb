{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c0eb2a-cf1a-4692-a9aa-206c299530d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c578398-6c93-4cdc-9312-367e78aca656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from io import StringIO\n",
    "from kmodes.kmodes import KModes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, fowlkes_mallows_score as FMI\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ad05d7-0df2-4778-af2c-c00393e412ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6ef7f7-14d8-4f8e-8b1b-a89cef28f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the Pandas display options for better readability\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419a408f-c704-405d-89f2-062adc5b6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets and their corresponding URLs\n",
    "dataset_urls = {\n",
    "    \"Soybean (Small)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\",\n",
    "    \"Zoo\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data\",\n",
    "    \"Heart Disease\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\",\n",
    "    \"Breast Cancer Wisconsin (Original)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",\n",
    "    \"Dermatology\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\",\n",
    "    \"Letter Recognition (E, F)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\",\n",
    "    \"Molecular Biology (Splice-junction Gene Sequences)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/splice-junction-gene-sequences/splice.data\",\n",
    "    \"Mushroom\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50c7dac-7020-4e68-b59c-c3d73885cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the true number of clusters for each dataset\n",
    "n_clusters_dict = {\n",
    "    \"Soybean (Small)\": 4,\n",
    "    \"Zoo\": 7,\n",
    "    \"Heart Disease\": 2,\n",
    "    \"Breast Cancer Wisconsin (Original)\": 2,\n",
    "    \"Dermatology\": 6,\n",
    "    \"Letter Recognition (E, F)\": 2,\n",
    "    \"Molecular Biology (Splice-junction Gene Sequences)\": 3,\n",
    "    \"Mushroom\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cfeaac-307e-4011-83ae-5a6a23b410a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of runs for benchmarking\n",
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aee7be-46f0-4849-bc93-b687d1a3b4be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading and Preprocessing of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab3beea-b230-4b8b-9003-8a622dd91799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}  # Dictionary to store cleaned dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1184017f-399f-4893-8c42-9c891d1351c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, url in dataset_urls.items():\n",
    "    response = requests.get(url, verify=False)\n",
    "    data = response.text\n",
    "    \n",
    "    # Convert the CSV/Text data into a DataFrame\n",
    "    data_io = StringIO(data)\n",
    "    df = pd.read_csv(data_io, header=None)\n",
    "    \n",
    "    df = df.replace('?', np.nan)  # Replace '?' with NaN\n",
    "    df = df.dropna()  # Drop rows with NaN values\n",
    "\n",
    "    # Set targets and features\n",
    "    if name == \"Letter Recognition (E, F)\":\n",
    "        y = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    elif name == \"Mushroom\":\n",
    "        y = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    elif name == \"Molecular Biology (Splice-junction Gene Sequences)\":\n",
    "        y = df.iloc[:, 0].str.strip()\n",
    "        X = pd.DataFrame([list(seq.strip()) for seq in df.iloc[:, 2]])\n",
    "    else:\n",
    "        X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "    # Drop columns with only 1 unique value\n",
    "    for col in X.columns:\n",
    "        if len(X[col].unique()) <= 1:\n",
    "            X.drop(columns=[col], inplace=True) # Diregard warning as it is behaving as expected\n",
    "    \n",
    "    # Store in the dataframes dictionary\n",
    "    dataframes[name] = {'features': X, 'targets': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac47f820-7855-4c26-abbb-b18a415093bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(dataframes):\n",
    "    if 'Zoo' in dataframes:\n",
    "        zoo_df = dataframes['Zoo']['features']\n",
    "        zoo_df = zoo_df.drop(columns=[0])\n",
    "        dataframes['Zoo']['features'] = zoo_df\n",
    "\n",
    "    if 'Heart Disease' in dataframes:\n",
    "        hd_df = dataframes['Heart Disease']['features']\n",
    "        columns_to_drop = [0, 3, 4, 7, 9]\n",
    "        hd_df = hd_df.drop(columns=hd_df.columns[columns_to_drop])\n",
    "        dataframes['Heart Disease']['features'] = hd_df\n",
    "        y_hd = dataframes['Heart Disease']['targets']\n",
    "        dataframes['Heart Disease']['targets'] = y_hd.apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    if 'Breast Cancer Wisconsin (Original)' in dataframes:\n",
    "        bcw_df = dataframes['Breast Cancer Wisconsin (Original)']['features']\n",
    "        bcw_df = bcw_df.drop(columns=bcw_df.columns[0])\n",
    "        dataframes['Breast Cancer Wisconsin (Original)']['features'] = bcw_df\n",
    "    \n",
    "    if 'Dermatology' in dataframes:\n",
    "        derm_df = dataframes['Dermatology']['features']\n",
    "        derm_df = derm_df.drop(columns=derm_df.columns[-1])\n",
    "        dataframes['Dermatology']['features'] = derm_df\n",
    "\n",
    "    if 'Letter Recognition (E, F)' in dataframes:\n",
    "        lr_ef_df = dataframes['Letter Recognition (E, F)']['features']\n",
    "        lr_ef_targets = dataframes['Letter Recognition (E, F)']['targets']\n",
    "        mask = lr_ef_targets.isin(['E', 'F'])\n",
    "        dataframes['Letter Recognition (E, F)']['features'] = lr_ef_df[mask]\n",
    "        dataframes['Letter Recognition (E, F)']['targets'] = lr_ef_targets[mask]\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33f0ed6-1950-4c9f-8872-7055e90233ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to datasets\n",
    "dataframes = preprocess_datasets(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c8072-b65f-481b-aa3c-35ac5972c630",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Bernoulli Mixture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571bf326-3c26-4fa3-b671-3a3094f5f9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BernoulliMixture:\n",
    "    def __init__(self, n_components, max_iter, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self,x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            # E-Step\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            # M-Step\n",
    "            self.get_Neff()\n",
    "            self.get_mu()\n",
    "            self.get_pi()\n",
    "            # Compute new log_likelihood:\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "\n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "\n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "\n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "\n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "\n",
    "    # Modified\n",
    "    def get_save_single(self, x, mu):\n",
    "        # Ensure x and mu are numpy arrays with float type\n",
    "        x = np.array(x, dtype=np.float64)\n",
    "        mu = np.array(mu, dtype=np.float64)\n",
    "\n",
    "        # Avoid taking log of zero by setting a minimum value\n",
    "        mu_place = np.where(mu <= 1e-15, 1e-15, mu)\n",
    "        # Perform the tensor dot product safely\n",
    "        try:\n",
    "            result = np.tensordot(x, np.log(mu_place), axes=(1, 1))\n",
    "        except TypeError as e:\n",
    "            print(\"TypeError encountered:\", e)\n",
    "            print(\"x shape:\", x.shape, \"x dtype:\", x.dtype)\n",
    "            print(\"mu_place shape:\", mu_place.shape, \"mu_place dtype:\", mu_place.dtype)\n",
    "            raise\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "\n",
    "    def get_mu(self):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, self.x) / self.Neff[:,None] \n",
    "\n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "\n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "\n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "\n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "\n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "\n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d75b725-227d-4e33-b50e-e040320fa2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bernoulli(features, n_components):\n",
    "    \"\"\"\n",
    "    Perform clustering using the Bernoulli Mixture Model.\n",
    "    Args:\n",
    "        features (np.array): The input features for clustering.\n",
    "        n_components (int): The number of components (clusters) in the mixture model.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The predicted cluster labels for each sample.\n",
    "    \"\"\"\n",
    "    # Initialize the Bernoulli Mixture model\n",
    "    bm = BernoulliMixture(n_components=n_components, max_iter=1000, tol=1e-3)\n",
    "    # Fit the model to the features\n",
    "    bm.fit(features)\n",
    "    # Predict the clusters\n",
    "    cluster_labels = bm.predict(features)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bb542-d0d1-4082-84d2-3718d090bdf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Stochastic Bernoulli Mixture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69e0ae5-b830-41b1-9f5b-f3938ad6119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticBernoulliMixture:\n",
    "    def __init__(self, n_components, max_iter, tol=1e-3, n_samples_per_component=10):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_samples_per_component = n_samples_per_component  # New parameter for SEM\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            # Stochastic E-step: draw a sample of latent variables and update gamma\n",
    "            self.stochastic_e_step()\n",
    "            self.remember_params()\n",
    "            # M-Step\n",
    "            self.get_Neff()\n",
    "            self.get_mu()\n",
    "            self.get_pi()\n",
    "            # Compute new log likelihood\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if abs(self.logL - self.old_logL) < self.tol:\n",
    "                break\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                break\n",
    "\n",
    "    def stochastic_e_step(self):\n",
    "        # Drawing a sample of z based on current parameters\n",
    "        log_probs = np.log(self.pi)[None, :] + self.get_log_bernoullis(self.x)\n",
    "        log_probs -= logsumexp(log_probs, axis=1)[:, None]\n",
    "        probs = np.exp(log_probs)\n",
    "        self.gamma = np.zeros_like(probs)\n",
    "        for i in range(self.x.shape[0]):\n",
    "            for _ in range(self.n_samples_per_component):\n",
    "                sampled_z = np.random.choice(self.n_components, p=probs[i])\n",
    "                self.gamma[i, sampled_z] += 1\n",
    "        self.gamma /= self.n_samples_per_component\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        # Compute the log probability for Bernoulli distribution\n",
    "        return np.einsum('ij,kj->ik', x, np.log(self.mu)) + np.einsum('ij,kj->ik', 1-x, np.log(1-self.mu))\n",
    "                \n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "\n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "\n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "\n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "\n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "\n",
    "    def get_save_single(self, x, mu):\n",
    "        # Ensure x and mu are numpy arrays with float type\n",
    "        x = np.array(x, dtype=np.float64)\n",
    "        mu = np.array(mu, dtype=np.float64)\n",
    "\n",
    "        # Avoid taking log of zero by setting a minimum value\n",
    "        mu_place = np.where(mu <= 1e-15, 1e-15, mu)\n",
    "        # Perform the tensor dot product safely\n",
    "        try:\n",
    "            result = np.tensordot(x, np.log(mu_place), axes=(1, 1))\n",
    "        except TypeError as e:\n",
    "            print(\"TypeError encountered:\", e)\n",
    "            print(\"x shape:\", x.shape, \"x dtype:\", x.dtype)\n",
    "            print(\"mu_place shape:\", mu_place.shape, \"mu_place dtype:\", mu_place.dtype)\n",
    "            raise\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "\n",
    "    def get_mu(self):\n",
    "        # Update mu based on the newly computed gamma\n",
    "        self.mu = np.einsum('ik,id->kd', self.gamma, self.x) / np.sum(self.gamma, axis=0)[:, None]\n",
    "\n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "\n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "\n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "\n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "\n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "\n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e203f449-a149-4ff4-94d6-b532087c2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_stochastic_bernoulli(features, n_components):\n",
    "    \"\"\"\n",
    "    Perform clustering using the Stochastic Bernoulli Mixture Model.\n",
    "    Args:\n",
    "        features (np.array): The input features for clustering.\n",
    "        n_components (int): The number of components (clusters) in the mixture model.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The predicted cluster labels for each sample.\n",
    "    \"\"\"\n",
    "    # Initialize the Stochastic Bernoulli Mixture model with specified number of samples per component\n",
    "    sbm = StochasticBernoulliMixture(n_components=n_components, max_iter=1000, tol=1e-3, n_samples_per_component=10)\n",
    "    # Fit the model to the features\n",
    "    sbm.fit(features)\n",
    "    # Predict the clusters\n",
    "    cluster_labels = sbm.predict(features)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0e828-df30-44f4-997e-491b9e22d10f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KModes Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acca7393-cf23-4568-9740-8149a376f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmodes(features, n_clusters):\n",
    "    \"\"\"Perform clustering using KModes algorithm.\"\"\"\n",
    "    km = KModes(n_clusters=n_clusters, init='random', n_init=5)\n",
    "    clusters = km.fit_predict(features)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1258b2-aa27-42c5-af17-7d0f77dfe9ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Execution of Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3b23dd-62b1-4b15-b70a-71a6cb502c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering_algorithms(dataframes, n_clusters_dict, num_runs=10):\n",
    "    results_list = []\n",
    "    for name, data in dataframes.items():\n",
    "        print(\"Processing:\", name)\n",
    "        \n",
    "        features = data['features']\n",
    "        true_labels = data['targets'].squeeze()  # Assuming targets are in a single column\n",
    "        n_clusters = n_clusters_dict.get(name, 2)  # Default to 2 clusters if not specified\n",
    "        \n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        features_encoded = encoder.fit_transform(features)\n",
    "\n",
    "        metrics = {'Bernoulli Mixture': [], 'Stochastic Bernoulli Mixture': [], 'KModes': []}  # Initialize a dictionary to store results for each method\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            # Bernoulli Mixture\n",
    "            bm_clusters = perform_bernoulli(features_encoded, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, bm_clusters)\n",
    "            metrics['Bernoulli Mixture'].append((ari, nmi, fmi))\n",
    "            \n",
    "            # Stochastic Bernoulli Mixture\n",
    "            sbm_clusters = perform_stochastic_bernoulli(features_encoded, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, sbm_clusters)\n",
    "            metrics['Stochastic Bernoulli Mixture'].append((ari, nmi, fmi))\n",
    "            \n",
    "            # KModes\n",
    "            km_clusters = perform_kmodes(features, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, km_clusters)\n",
    "            metrics['KModes'].append((ari, nmi, fmi))\n",
    "\n",
    "        # Calculate mean and standard deviation for each method and append to results list\n",
    "        for method, values in metrics.items():\n",
    "            ari_vals, nmi_vals, fmi_vals = zip(*values)\n",
    "            ari_mean, ari_std = np.mean(ari_vals), np.std(ari_vals)\n",
    "            nmi_mean, nmi_std = np.mean(nmi_vals), np.std(nmi_vals)\n",
    "            fmi_mean, fmi_std = np.mean(fmi_vals), np.std(fmi_vals)\n",
    "            results_list.append({\n",
    "                \"Dataset\": name,\n",
    "                \"Method\": method,\n",
    "                \"ARI\": f\"{ari_mean:.4f}±{ari_std:.2f}\",\n",
    "                \"NMI\": f\"{nmi_mean:.4f}±{nmi_std:.2f}\",\n",
    "                \"FMI\": f\"{fmi_mean:.4f}±{fmi_std:.2f}\"\n",
    "            })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame for results\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e02cba4-6145-42f5-8a69-8d581b09bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate clustering metrics: Adjusted Rand Index, Normalized Mutual Information, and Folkes-Mallows Index.\"\"\"\n",
    "    return ARI(true_labels, predicted_labels), NMI(true_labels, predicted_labels), FMI(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "002b217e-bdf7-4073-a889-46f738df3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Soybean (Small)\n",
      "Processing: Zoo\n",
      "Processing: Heart Disease\n",
      "Processing: Breast Cancer Wisconsin (Original)\n",
      "Processing: Dermatology\n",
      "Processing: Letter Recognition (E, F)\n",
      "Processing: Molecular Biology (Splice-junction Gene Sequences)\n",
      "Processing: Mushroom\n"
     ]
    }
   ],
   "source": [
    "# Running all algorithms and storing the results\n",
    "results = run_clustering_algorithms(dataframes, n_clusters_dict, num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d64ca-eb26-4808-95fd-936e1548c377",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Presentation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4d64b74-174f-4ad6-b77c-0038d1c8d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_results(results_df):\n",
    "    # Expanding 'ARI', 'NMI', and 'FMI' columns into multiple rows with a new 'Metric' column\n",
    "    expanded_df = pd.melt(results_df, id_vars=[\"Dataset\", \"Method\"], value_vars=[\"ARI\", \"NMI\", \"FMI\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    expanded_df[['Metric_Value', 'Std']] = expanded_df['Value'].str.split('±', expand=True)\n",
    "    expanded_df.drop(columns=['Value'], inplace=True)  # Removing the original combined column\n",
    "    \n",
    "    # Convert the 'Metric_Value' and 'Std' columns to numeric types\n",
    "    expanded_df['Metric_Value'] = expanded_df['Metric_Value'].astype(float)\n",
    "    expanded_df['Std'] = expanded_df['Std'].astype(float)\n",
    "\n",
    "    # Concatenate the metric value and standard deviation back into a single string\n",
    "    expanded_df['Metric_Value'] = expanded_df['Metric_Value'].map('{:.4f}'.format) + \"±\" + expanded_df['Std'].map('{:.2f}'.format)\n",
    "    \n",
    "    # Ensuring the order of datasets and methods remains consistent with the original DataFrame\n",
    "    dataset_order = results_df['Dataset'].unique()\n",
    "    method_order = results_df['Method'].unique()\n",
    "\n",
    "    # Creating a pivot table to restructure the DataFrame as required\n",
    "    pivot_df = expanded_df.pivot_table(index=[\"Dataset\", \"Metric\"], columns=\"Method\", values=\"Metric_Value\", aggfunc='first')\n",
    "    \n",
    "    # Reindexing the pivot table to maintain the original order\n",
    "    pivot_df = pivot_df.reindex(dataset_order, level='Dataset')\n",
    "    pivot_df = pivot_df.reindex(method_order, axis='columns')\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fa7370e-a347-4b4a-be9a-a065833ed6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the results\n",
    "formatted_results = reformat_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a164506-c0b3-40cd-acf6-f734a06c78b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                                    Bernoulli Mixture Stochastic Bernoulli Mixture       KModes\n",
      "Dataset                                            Metric                                                            \n",
      "Soybean (Small)                                    ARI          1.0000±0.00                  0.9356±0.09  0.9143±0.07\n",
      "                                                   FMI          1.0000±0.00                  0.9546±0.06  0.9355±0.05\n",
      "                                                   NMI          1.0000±0.00                  0.9543±0.05  0.9376±0.04\n",
      "Zoo                                                ARI          0.6972±0.00                  0.6515±0.06  0.6784±0.12\n",
      "                                                   FMI          0.7645±0.00                  0.7282±0.05  0.7497±0.09\n",
      "                                                   NMI          0.8028±0.00                  0.7790±0.02  0.7929±0.05\n",
      "Heart Disease                                      ARI          0.3292±0.00                  0.3793±0.01  0.3415±0.03\n",
      "                                                   FMI          0.6646±0.00                  0.6896±0.00  0.6730±0.02\n",
      "                                                   NMI          0.2598±0.00                  0.3014±0.01  0.2654±0.02\n",
      "Breast Cancer Wisconsin (Original)                 ARI          0.8800±0.00                  0.8806±0.00  0.7542±0.05\n",
      "                                                   FMI          0.9445±0.00                  0.9448±0.00  0.8917±0.02\n",
      "                                                   NMI          0.8152±0.00                  0.8158±0.00  0.6484±0.04\n",
      "Dermatology                                        ARI          0.7718±0.00                  0.7846±0.06  0.4829±0.07\n",
      "                                                   FMI          0.8188±0.00                  0.8279±0.05  0.5894±0.06\n",
      "                                                   NMI          0.8291±0.00                  0.8136±0.04  0.6358±0.08\n",
      "Letter Recognition (E, F)                          ARI          0.0021±0.00                  0.0425±0.01  0.2196±0.03\n",
      "                                                   FMI          0.5012±0.00                  0.5369±0.00  0.6204±0.02\n",
      "                                                   NMI          0.0020±0.00                  0.0342±0.01  0.1831±0.03\n",
      "Molecular Biology (Splice-junction Gene Sequences) ARI          0.8589±0.00                  0.8364±0.00  0.0268±0.01\n",
      "                                                   FMI          0.9132±0.00                  0.8994±0.00  0.3768±0.01\n",
      "                                                   NMI          0.7779±0.00                  0.7527±0.00  0.0405±0.01\n",
      "Mushroom                                           ARI          0.5111±0.00                  0.4921±0.02  0.4846±0.01\n",
      "                                                   FMI          0.7915±0.00                  0.7815±0.01  0.7845±0.00\n",
      "                                                   NMI          0.4538±0.00                  0.4231±0.04  0.4486±0.01\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted results\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3d880-9149-43e7-a241-141a61bfe2e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Performance Comparison and Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e5972-9a7c-4c2b-9fdf-2dfaa0f6be5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### General Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23174d58-afe9-4a09-91bb-2f004975a8ad",
   "metadata": {},
   "source": [
    "- The Bernoulli Mixture generally performs consistently well across all three metrics, often achieving the highest or near-highest scores.\n",
    "- The Stochastic Bernoulli Mixture is competitive, often close to Bernoulli Mixture, indicating a slight variation due to its stochastic nature.\n",
    "- KModes shows varying performance, performing comparably in simpler datasets but lagging in more complex or larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7cdc9-1d45-4838-82a7-1c3d082e628a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Why is FMI consistently higher than ARI and NMI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fdcac-01c3-406c-826d-c048c0473969",
   "metadata": {},
   "source": [
    "- **FMI** measures the geometric mean of precision and recall, focusing on the number of correct decisions (true positive) over all pairs. It is generally less sensitive to the actual grouping (cluster purity and recovery), which can make it appear higher as it emphasizes agreement over all pairs.\n",
    "- **ARI** adjusts for chance in the clustering performance, providing a more stringent measure that evaluates both the true positive and negative decisions against possible random chance. This can result in lower scores if the clustering contains many splits or mergers not aligned with true labels.\n",
    "- **NMI** evaluates mutual information in relation to the clusters' and ground truth's entropies, adjusted for chance. It's sensitive to the actual amount of shared information between the clustering result and the true labels, which can be lower if clusters are not perfectly informative regarding true groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae25be7-ed55-4b3f-9c0a-d3a706a79053",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Why are ARI and NMI often lower compared to FMI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56df667-b80d-4d7c-b6b3-73f26614aa0d",
   "metadata": {},
   "source": [
    "- Both ARI and NMI are more sensitive to the actual alignment and information shared between the predicted clusters and the true labels. They penalize both over-segmentation and under-segmentation more significantly than FMI. ARI, being adjusted for chance, reflects a more conservative estimate of clustering quality, often resulting in lower scores especially when clusters are not well-separated or are uneven. NMI similarly reflects discrepancies in informational alignment, which can result in lower values if clustering does not capture the natural groups effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04be55-7062-404b-97e6-d360911ff107",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Dataset Complexity and Clustering Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b0057-85f3-4e74-a88b-fb39211d1aed",
   "metadata": {},
   "source": [
    "- Datasets like \"Soybean (Small)\" and \"Breast Cancer Wisconsin\" with high scores across methods suggest that these datasets have clear, distinguishable clusters that all methods can identify well.\n",
    "- Datasets with mixed or lower scores, like \"Heart Disease\" or \"Molecular Biology,\" suggest more complex data structures or less distinct clustering patterns, challenging for methods, especially non-stochastic ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
